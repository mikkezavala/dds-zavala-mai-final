---
title: "DDS-6306 Wine Quality - Final Project"
output: html_document
date: "2024-11-27"
author: "Miguel Zavala, Tommy Mai"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Wine Quality

## Environment Preparation

```{r}
library(tidyverse)
library(caret)
library(class)
library(e1071)
library(corrplot)
library(FNN)
library(car)
library(GGally)
library(naivebayes)
library(ggcorrplot)
library(dplyr)
library(furrr)
library(parallel)
library(progressr)
library(DT)
```

## Data Inspection

```{r}

wine_train = read.csv('./Data/Wine_Train.csv', header = TRUE)
wine_test = read.csv("./Data/Wine_Test_Set.csv", header = TRUE) 
wine_type_location = read.csv('./Data/Wine_Types_and_Locations.csv', header = TRUE)

wine_test <- wine_test |> mutate(location = ifelse(location == "Califormia", "California", location))

type_mode <- tolower(names(sort(table(wine_type_location$type), decreasing = TRUE))[1])
location_mode <- tolower(names(sort(table(wine_type_location$location), decreasing = TRUE))[1])

wine_type_location <- wine_type_location |> mutate(
  type = ifelse(is.na(type) | type == "", type_mode, tolower(type)),
  location = ifelse(
    is.na(location) |
      location == "",
    location_mode,
    tools::toTitleCase(location)
  )
) |>
  mutate(location = ifelse(location == "Califormia", "California", location)) |>
  mutate(type = factor(type), location = factor(location))

wine_train <- left_join(wine_train, wine_type_location, by = "ID")

sprintf("Data composed of: %d rows and %d columns",
        nrow(wine_train),
        ncol(wine_train))

head(wine_train, n = 5)
str(wine_train)

emptyData <- colSums(is.na(wine_train))
totalRows <- nrow(wine_train)

tibble(
  Column = names(wine_train),
  `Total_Rows` = totalRows,
  `NA_Count` = emptyData
)
```

### Histogram of the quality

```{r}
ggplot(wine_train, aes(x = factor(quality))) + 
  geom_bar(fill = "purple", color = "black", alpha = 0.7) +  
  geom_text(                                           
    stat = "count", 
    aes(label = paste0(..count.., " (", round(..count.. / sum(..count..) * 100, 1), "%)")),
    vjust = -0.5, 
    size = 5
  ) +
  scale_x_discrete(breaks = sort(unique(wine_train$quality))) +
  labs(
    title = "Distribution of Wine Quality", 
    x = "Quality", 
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"), 
    axis.text.x = element_text(size = 12)                      
  )
```

### Remove 'ID' column

```{r}
correlation_matrix = cor(wine_train %>%
                           select_if(is.numeric) %>%
                           select(-ID))  # Exclude the 'ID' column

# Correlation matrix
corrplot(correlation_matrix, 
         method = "color",           
         type = "upper",             
         tl.col = "black",         
         tl.srt = 45,                
         addCoef.col = "black",      
         number.cex = 0.7,           
         col = colorRampPalette(c("red", "white", "blue"))(200),  
         diag = FALSE,               
         tl.cex = 0.8)
```

### Variable Inflation

```{r}
vif_data = lm(quality ~ ., data = wine_train %>% select(-ID))
vif_values = vif(vif_data)
print(vif_values)
```

#### Addressing High Inflation

```{r}
high_vif_threshold = 10
features_to_remove = names(vif_values[vif_values > high_vif_threshold])
model_cols = setdiff(names(wine_train %>% select_if(is.numeric)), c("quality", features_to_remove))
sprintf("Removed features with high VIF: %s", features_to_remove)
```

## Feature Construction

### Top Features

```{r}
numeric_features = wine_train %>% select_if(is.numeric)
correlation_matrix = cor(numeric_features)
quality_correlation = correlation_matrix["quality", ]
top_3_features = sort(quality_correlation, decreasing = TRUE)[2:4] 
top_3_feature_names = names(top_3_features)
print(top_3_features) 

# Boxplot for Alcohol by Quality
ggplot(wine_train, aes(x = factor(quality), y = alcohol, fill = factor(quality))) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Boxplot of Alcohol Content by Wine Quality",
       x = "Quality Score",
       y = "Alcohol Content",
       fill = "Quality") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

#### Wine Types

```{r}
ggplot(wine_train, aes(x = type, fill = type)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 5) + 
  scale_fill_manual(
    values = c("red" = "maroon", "white" = "ivory")  # Define colors
  ) +
  labs(
    title = "Types of Wine (Imputed Unknowns)", 
    x = "Type", 
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text = element_text(size = 12)
  )
```

#### Wine Locations

```{r}
ggplot(wine_train, aes(x = location, fill = location)) +
  geom_bar(color = "black") +  
  geom_text(
    stat = "count",
    aes(label = ..count..),
    vjust = -0.5, size = 5, color = "black"
  ) +
  scale_fill_manual(
    values = c("Texas" = "darkred", "California" = "darkblue")
  ) +
  labs(
    title = "Location Count",         
    x = "Location",                           
    y = "Count"
  ) +                       
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

### Naive Bayes and Linear Regression

#### Prepare environment and helper functions

```{r}
n_cores <- detectCores()
max_memory <- 16 * 1024 ^ 3
plan(multisession, workers = max(1, n_cores - 2))
options(future.globals.maxSize = max_memory)

evaluateModels <- function(target, features, data_train) {
  train_index <- createDataPartition(data_train[[target]], p = 0.7, list = FALSE)
  train_data <- data_train[train_index, ]
  test_data <- data_train[-train_index, ]
  
  train_x <- train_data[, features, drop = FALSE]
  test_x <- test_data[, features, drop = FALSE]
  train_labels <- factor(train_data[[target]])
  test_labels <- factor(test_data[[target]], levels = levels(train_labels))
  
  ## NB
  formulaNb <- paste(target, "~", paste(features, collapse = " + "))
  nbModel <- naiveBayes(as.formula(formulaNb), data = train_data)
  
  nbPredictions <- predict(nbModel, test_data)
  nbMatrix <- confusionMatrix(factor(nbPredictions, levels = levels(test_labels)), test_labels, mode = "everything")
  
  return(
    list(
      nb_formula = formulaNb,
      nb_accuracy = nbMatrix$overall["Accuracy"],
      nb_sensitivity = mean(nbMatrix$byClass[, "Sensitivity"], na.rm = TRUE),
      nb_specificity = mean(nbMatrix$byClass[, "Specificity"], na.rm = TRUE)
    )
  )
}

evaluateRegression <- function(target, features, data) {
  model <- paste(target, " ~ ", paste(features, collapse = " + "))
  train_indices <- createDataPartition(data[[target]], p = 0.75, list = FALSE)
  train_data <- data[train_indices, ]
  validation_data <- data[-train_indices, ]
  
  fit <- lm(as.formula(model), data = train_data)
  predictions <- predict(fit, newdata = validation_data)
  actual_values <- validation_data[[target]]
  
  residuals <- actual_values - predictions
  mae_value <- mean(abs(residuals))
  
  return(
    list(
      features = model,
      AIC = AIC(fit),
      BIC = BIC(fit),
      MAE = mae_value,
      RMSE = sqrt(mean(residuals ^ 2)),
      ADJUSTED_R2 = summary(fit)$adj.r.squared,
      R2_VALIDATION = 1 - sum(residuals ^ 2) / sum((actual_values - mean(actual_values)) ^ 2)
    )
  )
}

permuteModels <- function(target, data) {
  all_features <- colnames(data)[!colnames(data) %in% target]
  total_features <- length(all_features)
  
  all_combinations <- unlist(lapply(1:total_features, function(size) {
    combn(all_features, size, simplify = FALSE)
  }), recursive = FALSE)
  
  chunk_size <- ceiling(length(all_combinations) / future::nbrOfWorkers())
  chunks <- split(all_combinations, ceiling(seq_along(all_combinations) / chunk_size))
  
  total_steps <- length(all_combinations)
  p <- progressr::progressor(steps = total_steps)
  
  print(paste("Will Run --- ", total_steps, " --- Combinations", collapse = "\n"))
  results <- future_map_dfr(seq_along(chunks), ~ {
    chunk <- chunks[[.x]]
    results_list <- list()
    
    for (i in seq_along(chunk)) {
      features_subset <- chunk[[i]]
      # message(paste("Starting: ", target, " ~ ", paste(features_subset, collapse = " + "), collapse = "\n"))
      knn_nb_results <- data.frame(evaluateModels(target, features_subset, data))
      regression_results <- data.frame(evaluateRegression(target, features_subset, data))
      
      #regression_results <- regression_results[regression_results$adjusted_r2 > 0.7, , drop = FALSE]
      combined_results <- data.frame(
        iteration = paste(.x, "-", i),
        features = paste(features_subset, collapse = ", "),
        nb_accuracy = knn_nb_results$nb_accuracy,
        nb_sensitivity = knn_nb_results$nb_sensitivity,
        nb_specificity = knn_nb_results$nb_specificity,
        AIC = regression_results$AIC,
        BIC = regression_results$BIC,
        MAE = regression_results$MAE,
        RMSE = regression_results$RMSE,
        ADJUSTED_R2 = regression_results$ADJUSTED_R2,
        R2_VALIDATION = regression_results$R2_VALIDATION
      )
      
      results_list[[i]] <- combined_results
      p(paste("Chunk", .x, "complete"))
    }
    
    
    do.call(rbind, results_list)
  }, .options = furrr_options(seed = TRUE))
  
  return(results)
}

runTests <- function(target, data) {
  progressr::with_progress({
    all_results <- permuteModels(target, data)
  })
  
  return(all_results |>
           arrange(MAE) |>
           mutate(across(where(is.numeric), ~ round(.x, 6))))
}

```

#### Excute combinations and display the results

```{r}
results <- runTests("quality", wine_train |> dplyr::select(-ID, -type, -location))
datatable(
  results,
  caption = "Performance Comparison for NB, KNN, and Regression Models",
  options = list(pageLength = 5, autoWidth = TRUE),
  rownames = FALSE
)
```

### Compare with polynomial models

#### Helper functions

```{r}
test_polynomials <- function(data, base_formula, features) {
  results <- data.frame()
  
  for (feature in features) {
    formula_quad <- paste(base_formula, " + I(", feature, "^2)", sep = "")
    formula_cubic <- paste(base_formula, "+ I(", feature, "^2) + I(", feature, "^3)", sep = "")
    
    fit_quad <- lm(as.formula(formula_quad), data = data)
    fit_cubic <- lm(as.formula(formula_cubic), data = data)
    
    mse_quad <- mean(residuals(fit_quad)^2)
    adj_r2_quad <- summary(fit_quad)$adj.r.squared
    
    mse_cubic <- mean(residuals(fit_cubic)^2)
    adj_r2_cubic <- summary(fit_cubic)$adj.r.squared
    
    analysis <- data.frame(
      Feature = feature,
      AIC_QUAD = AIC(fit_quad),
      AIC_CUBIC = AIC(fit_cubic),
      MSE_QUAD = mse_quad,
      MSE_CUBIC = mse_cubic,
      ADJ_R2_QUAD = adj_r2_quad,
      ADJ_R2_CUBIC = adj_r2_cubic,
      FORMULA_CUBIC = formula_cubic,
      FORMULA_QUAD = formula_quad
    )
    results <- rbind(results, analysis)
  }
  
  return(results)
}
```

### Execute adding polynomials and compare

```{r}
model_base <- "quality ~ volatile.acidity + citric.acid + residual.sugar + chlorides + pH + alcohol + sulphates"
# fixed.acidity + citric.acid + chlorides + residual.sugar + pH + alcohol + sulphates + type + location + type * citric.acid  + type * location
features <- c(
  "volatile.acidity",
  "citric.acid",
  "residual.sugar",
  "chlorides",
  "pH",
  "alcohol",
  "sulphates"
)


#volatile.acidity, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, sulphates, alcohol

results <- test_polynomials(wine_train, model_base, features)
results <- results[order(results$MSE_QUAD), ]
results
```

### Fitting with Linear Regression

```{r}
set.seed(1233)
train_index <- createDataPartition(wine_train[["quality"]], p = 0.7, list = FALSE)
train_set <- wine_train[train_index, ]
test_set <- wine_train[-train_index, ]

model <- quality ~ fixed.acidity + citric.acid + chlorides + residual.sugar + pH + alcohol + sulphates + type + location + type * location
fit <- lm(model, data = train_set)

summary_fit <- summary(fit)
actual_lm <- as.numeric(as.character(test_set$quality))
predicted_quality <- predict(fit, newdata = test_set)
lm_mae <- mean(abs(actual_lm - predicted_quality))

sprintf("MAE: %.6f -- ADJ RSQR: %.6f -- AIC: %.6f",lm_mae, summary_fit$adj.r.squared, AIC(fit))
```

### Fitting with Naive Bayes

```{r}
cv_control = trainControl(method = "cv", number = 4)
train_index <- createDataPartition(wine_train[["quality"]], p = 0.7, list = FALSE)
train_set <- wine_train[train_index, ]
test_set <- wine_train[-train_index, ]

train_set$quality <- factor(train_set$quality)
test_set$quality <- factor(test_set$quality, levels = levels(train_set$quality))
nb_fit <- train(
  quality ~ fixed.acidity + citric.acid + chlorides + residual.sugar + pH + alcohol + sulphates + type + location,
  data = train_set,
  method = "naive_bayes",
  trControl = cv_control
  )

```

#### Predictions and MAE for Naive Bayes

```{r}
test_set$quality <- factor(test_set$quality, levels = levels(train_set$quality))
predictions_prob <- predict(nb_fit, newdata = test_set, type = "prob")

quality_levels <- as.numeric(levels(test_set$quality))
predicted_numeric <- apply(predictions_prob, 1, function(x) sum(x * quality_levels))
actual_numeric <- as.numeric(as.character(test_set$quality))
nb_mae <- mean(abs(actual_numeric - predicted_numeric))
sprintf("Mean Absolute Error (MAE): %.6f", nb_mae)
```

## Model MAE comparison

```{r}
model_comparison = data.frame(
  Model = c("Linear Regression", "Naive Bayes"),
  MAE = c(lm_mae, nb_mae)
)
print(model_comparison)

mae_data = data.frame(
  Model = c("Linear Regression", "Naive Bayes"),
  MAE = c(lm_mae, nb_mae)
)

ggplot(model_comparison, aes(x = Model, y = MAE, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(MAE, 4)), vjust = -0.5, size = 4, color = "black") +
  labs(
    title = "Model Comparison by MAE",
    x = "Model",
    y = "Mean Absolute Error"
  ) +
  theme_minimal()
```

### Define model_cols without 'ID' and 'quality'

```{r}
model_cols = setdiff(names(wine_train %>% select_if(is.numeric)), c("ID", "quality"))
feature_correlations_knn = cor(wine_train[, model_cols], wine_train$quality, use = "complete.obs")

feature_importance_knn = data.frame(
  Feature = model_cols,
  Correlation = feature_correlations_knn
) %>% 
  arrange(desc(abs(Correlation)))

ggplot(feature_importance_knn, aes(x = reorder(Feature, Correlation), y = Correlation, fill = Correlation > 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("red", "blue"), labels = c("Negative", "Positive")) +
  labs(
    title = "Feature Importance for k-NN (Correlation with Quality)",
    x = "Feature",
    y = "Correlation",
    fill = "Direction"
  ) +
  theme_minimal()
```

## Create submission file with ID and predicted Quality

```{r}
create_submission_file = function(test_data, predictions, file_path) {
  submission = data.frame(
    ID = test_data$ID,
    Quality = predictions
  )
  write.csv(submission, file_path, row.names = FALSE)
  sprintf("Submission file created successfully at: %s", file_path)
}
```

### Build Predictions on Test Set

```{r}
wine_test$quality <- predict(nb_fit, newdata = wine_test)
```

### Define the file path and call the function

```{r}
file_path = "Wine_Test_Predictions.csv"
write.csv(
  wine_test |> dplyr::select(ID, quality) |>
    mutate(quality = as.numeric(quality)),
  file_path,
  row.names = FALSE
)
```
