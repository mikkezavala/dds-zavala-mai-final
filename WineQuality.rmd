---
title: "DDS 6306 Wine Quality - Final Project"
output: html_document
date: "2024-11-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(corrplot)

wines = read.csv(
  './Wine_Train.csv',
  header = TRUE
)

sprintf("Data composed of: %d rows and %d columns",
        nrow(wines),
        ncol(wines))
head(wines, n = 5)

emptyData <- colSums(is.na(wines))
totalRows <- nrow(wines)

tibble(Column = names(wines),
       `Total_Rows` = totalRows,
       `NA_Count` = emptyData)

selected_data <- wines |> select(-ID)
corr_matrix <- cor(selected_data, use = "complete.obs")

corrplot(corr_matrix, method = "color")

```

## Detecting correlation (High / Low)

After observing the variables correlation, some of the predictor variables are correlated, with the potential of producing collinearity and questioning independence, which we will asses later in more detail, following predictors present this behavior:

-   Density + Residual Sugar (0.55)

-   Density + Alcohol (-0.7)

-   Total Sulfur Dioxide + Residual Sugar (0.5)

-   Total Sulfur Dioxide + Free Sulfur Dioxide (0.72)

At first sigh without the intention to make inference an interest relationship in the train set found is quality has some positive correlation with alcohol (0.44), and a negative relevant correlation with density (-0.30). As an important clarification this only displaying the correlation and not making any attempt for inferences.

## Feature Selection

```{r}
library(e1071)
library(parallel)
library(progressr)
library(furrr)
library(caret)
library(DT)
library(Metrics) 


# Parallel Execution Setup
n_cores <- detectCores()
max_memory <- 16 * 1024 ^ 3
plan(multisession, workers = max(1, n_cores - 2))
options(future.globals.maxSize = max_memory)


evaluateModels <- function(target, features, data_train, knn_n = 5) {
  ### KNN
  train_index <- createDataPartition(data_train[[target]], p = 0.7, list = FALSE)
  train_data <- data_train[train_index, ]
  test_data <- data_train[-train_index, ]
  
  train_x <- train_data[, features, drop = FALSE]
  test_x <- test_data[, features, drop = FALSE]
  train_labels <- factor(train_data[[target]])
  test_labels <- factor(test_data[[target]], levels = levels(train_labels))
  
  knnModel <- FNN::knn(
    train = train_x,
    test = test_x,
    cl = train_labels,
    k = knn_n
  )
  
  knnMatrix <- confusionMatrix(factor(knnModel, levels = levels(test_labels)), test_labels, mode = "everything")
  
  ## NB
  formulaNb <- paste(target, "~", paste(features, collapse = " + "))
  nbModel <- naiveBayes(as.formula(formulaNb), data = train_data)
  
  nbPredictions <- predict(nbModel, test_data)
  nbMatrix <- confusionMatrix(factor(nbPredictions, levels = levels(test_labels)), test_labels, mode = "everything")
  
  return(
    list(
      nb_formula = formulaNb,
      nb_accuracy = nbMatrix$overall["Accuracy"],
      nb_sensitivity = mean(nbMatrix$byClass[, "Sensitivity"], na.rm = TRUE),
      nb_specificity = mean(nbMatrix$byClass[, "Specificity"], na.rm = TRUE),
      knn_accuracy = knnMatrix$overall["Accuracy"],
      knn_sensitivity = mean(knnMatrix$byClass[, "Sensitivity"], na.rm = TRUE),
      knn_specificity = mean(knnMatrix$byClass[, "Specificity"], na.rm = TRUE)
    )
  )
}

evaluateRegression <- function(target, features, data) {
  model <- paste(target, " ~ ", paste(features, collapse = " + "))
  train_indices <- createDataPartition(data[[target]], p = 0.75, list = FALSE)
  train_data <- data[train_indices, ]
  validation_data <- data[-train_indices, ]
  
  fit <- lm(as.formula(model), data = train_data)
  predictions <- predict(fit, newdata = validation_data)
  actual_values <- validation_data[[target]]
  
  residuals <- actual_values - predictions
  mae_value <- mean(abs(residuals))
  
  # Metrics::mae(validation_data[[target]], predictions)
  # mean(abs(residuals))
  # print(validation_data[[target]])

  
  return(
    list(
      features = model,
      AIC = AIC(fit),
      BIC = BIC(fit),
      MAE = mae_value,
      RMSE = sqrt(mean(residuals ^ 2)),
      ADJUSTED_R2 = summary(fit)$adj.r.squared,
      R2_VALIDATION = 1 - sum(residuals ^ 2) / sum((actual_values - mean(actual_values)) ^ 2)
    )
  )
}

library(dplyr)
library(furrr)
library(progressr)
permuteModels <- function(target, data, k = 5) {
  all_features <- colnames(data)[!colnames(data) %in% target]
  total_features <- length(all_features)
  
  all_combinations <- unlist(lapply(1:total_features, function(size) {
    combn(all_features, size, simplify = FALSE)
  }), recursive = FALSE)
  
  chunk_size <- ceiling(length(all_combinations) / future::nbrOfWorkers())
  chunks <- split(all_combinations, ceiling(seq_along(all_combinations) / chunk_size))
  
  total_steps <- length(all_combinations)
  p <- progressr::progressor(steps = total_steps)
  
  print(paste("Will Run --- ", total_steps, " --- Combinations", collapse = "\n"))
  results <- future_map_dfr(seq_along(chunks), ~ {
    chunk <- chunks[[.x]]
    results_list <- list()
    
    for (i in seq_along(chunk)) {
      features_subset <- chunk[[i]]
      # message(paste("Starting: ", target, " ~ ", paste(features_subset, collapse = " + "), collapse = "\n"))
      knn_nb_results <- data.frame(evaluateModels(target, features_subset, data, k))
      regression_results <- data.frame(evaluateRegression(target, features_subset, data))
      
      #regression_results <- regression_results[regression_results$adjusted_r2 > 0.7, , drop = FALSE]
      combined_results <- data.frame(
        iteration = paste(.x, "-", i),
        features = paste(features_subset, collapse = ", "),
        nb_accuracy = knn_nb_results$nb_accuracy,
        nb_sensitivity = knn_nb_results$nb_sensitivity,
        nb_specificity = knn_nb_results$nb_specificity,
        knn_accuracy = knn_nb_results$knn_accuracy,
        knn_sensitivity = knn_nb_results$knn_sensitivity,
        knn_specificity = knn_nb_results$knn_specificity,
        AIC = regression_results$AIC,
        BIC = regression_results$BIC,
        MAE = regression_results$MAE,
        RMSE = regression_results$RMSE,
        ADJUSTED_R2 = regression_results$ADJUSTED_R2,
        R2_VALIDATION = regression_results$R2_VALIDATION
      )
      
      results_list[[i]] <- combined_results
      p(paste("Chunk", .x, "complete"))
    }
    
    
    do.call(rbind, results_list)
  }, .options = furrr_options(seed = TRUE))
  
  return(results)
}

runTests <- function(target, data, k = 30) {
  progressr::with_progress({
    all_results <- permuteModels(target, data, k)
  })
  
  return(all_results |>
           arrange(MAE) |>
           mutate(across(where(is.numeric), ~ round(.x, 6))))
}

results <- runTests("quality", wines |> dplyr::select(-all_of(c("ID"))), 10)

datatable(
  results,
  caption = "Performance Comparison for NB, KNN, and Regression Models",
  options = list(pageLength = 20, autoWidth = TRUE),
  rownames = FALSE
)

```
