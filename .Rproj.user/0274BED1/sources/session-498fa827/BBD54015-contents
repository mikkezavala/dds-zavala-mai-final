# Load necessary libraries
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(caret)
library(e1071)
library(class)
library(furrr)
library(future)
library(progressr)
library(forcats)
library(parallel)
library(knitr)
library(purrr)
library(DT)

# Load Data
data_train <- read.csv(
  '/Users/mikkezavala/Library/Mobile Documents/com~apple~CloudDocs/Documents/School/Master in Data Science/[DS-6371] Statistical Foundations for Data Science/Final Project/train.csv'
)

sprintf("Data composed of: %d rows and %d columns",
        nrow(data_train),
        ncol(data_train))

# Filter Columns
filtered_columns <- c(
  "LotFrontage",
  "Alley",
  "BsmtQual",
  "BsmtCond",
  "BsmtExposure",
  "BsmtFinType1",
  "BsmtFinType2",
  "FireplaceQu",
  "GarageType",
  "GarageYrBlt",
  "GarageFinish",
  "GarageQual",
  "PoolQC",
  "Fence",
  "MiscFeature"
)

filtered_data <- data_train |>
  select(-all_of(filtered_columns)) |>
  mutate(
    across(.cols = -SalePrice, .fns = as.factor),
    SalePriceBin = cut(
      SalePrice,
      breaks = c(-Inf, 100000, 150000, 200000, 300000, 400000, Inf),
      labels = c("Very Low", "Low", "Medium", "High", "Very High", "Luxury"),
      include.lowest = TRUE
    )
  )

# Parallel Execution Setup
n_cores <- detectCores()
max_memory <- 16 * 1024 ^ 3
plan(multisession, workers = max(1, n_cores - 2))
options(future.globals.maxSize = max_memory)

### Build Features Function ###
buildFeatures <- function(trainData, testData, target) {
  trainData <- trainData |> filter(!is.na(.data[[target]]))
  for (col in colnames(trainData)) {
    if (is.factor(trainData[[col]]) && col %in% colnames(testData)) {
      testData[[col]] <- factor(testData[[col]], levels = levels(trainData[[col]]))
    }
  }
  
  trainData <- trainData |> mutate(across(where(is.factor), ~ fct_na_value_to_level(.x, "Missing")))
  testData <- testData |> mutate(across(where(is.factor), ~ fct_na_value_to_level(.x, "Missing")))
  
  valid_factors <- sapply(trainData, function(col)
    ! (is.factor(col) && length(unique(col)) < 2))
  trainData <- trainData[, valid_factors, drop = FALSE]
  testData <- testData[, valid_factors, drop = FALSE]
  
  trainEncode <- model.matrix( ~ . - 1, data = trainData)
  testEncode <- model.matrix( ~ . - 1, data = testData)
  
  return(
    list(
      trainEncode = as.data.frame(trainEncode),
      testEncode = as.data.frame(testEncode),
      trainData = trainData,
      testData = testData
    )
  )
}

compute_knn <- function(trainEncode, testEncode, trainLabels, k) {
  knn_result <- FNN::knn(trainEncode, testEncode, trainLabels, k = k)
  return(knn_result)
}

evaluateModels <- function(features, target, k = 5) {
  trainIndex <- createDataPartition(filtered_data[[target]], p = 0.7, list = FALSE)
  trainData <- filtered_data[trainIndex, ]
  testData <- filtered_data[-trainIndex, ]
  
  splitFeatures <- buildFeatures(trainData, testData, target)
  trainEncode <- splitFeatures$trainEncode
  testEncode <- splitFeatures$testEncode
  trainLabels <- factor(trainData[[target]])
  testLabels <- factor(testData[[target]], levels = levels(trainLabels))
  
  if (nrow(trainEncode) != length(trainLabels))
    stop("Mismatch: trainEncode rows and trainLabels length differ.")
  if (nrow(testEncode) != length(testLabels))
    stop("Mismatch: trainEncode rows and trainLabels length differ.")
  
  knnModel <- compute_knn(trainEncode, testEncode, trainLabels, k = k)
  knnMatrix <- confusionMatrix(factor(knnModel, levels = levels(testLabels)), testLabels, mode = "everything")
  
  nbModel <- naiveBayes(as.formula(paste(
    target, "~", paste(features, collapse = " + ")
  )), data = trainData)
  nbPredictions <- predict(nbModel, testData)
  nbMatrix <- confusionMatrix(factor(nbPredictions, levels = levels(testLabels)), testLabels, mode = "everything")
  
  computed <- list(
    nb_accuracy = nbMatrix$overall["Accuracy"],
    nb_sensitivity = mean(nbMatrix$byClass[, "Sensitivity"], na.rm = TRUE),
    nb_specificity = mean(nbMatrix$byClass[, "Specificity"], na.rm = TRUE),
    knn_accuracy = knnMatrix$overall["Accuracy"],
    knn_sensitivity = mean(knnMatrix$byClass[, "Sensitivity"], na.rm = TRUE),
    knn_specificity = mean(knnMatrix$byClass[, "Specificity"], na.rm = TRUE)
  )
  
  return(computed)
}

evaluate_regression <- function(features, data_train) {
  formula <- as.formula(paste("SalePrice ~", paste(features, collapse = " + ")))
  model <- lm(formula, data = data_train)
  results <- tibble(
    features = paste(features, collapse = ", "),
    nb_accuracy = NA, nb_sensitivity = NA, nb_specificity = NA,
    knn_accuracy = NA, knn_sensitivity = NA, knn_specificity = NA,
    adjusted_r2 = summary(model)$adj.r.squared,
    AIC = AIC(model),
    BIC = BIC(model)
  )
  return(results)
}

evaluate_fixed_models <- function(data_train) {
  model_simple <- lm(SalePrice ~ GrLivArea, data = data_train)
  model_fixed <- lm(SalePrice ~ GrLivArea + FullBath, data = data_train)
  
  results <- tibble(
    features = c("Simple Linear Regression (GrLivArea)",
                 "Fixed Multiple Regression (GrLivArea + FullBath)"),
    nb_accuracy = NA, nb_sensitivity = NA, nb_specificity = NA,
    knn_accuracy = NA, knn_sensitivity = NA, knn_specificity = NA,
    adjusted_r2 = c(summary(model_simple)$adj.r.squared,
                    summary(model_fixed)$adj.r.squared),
    AIC = c(AIC(model_simple), AIC(model_fixed)),
    BIC = c(BIC(model_simple), BIC(model_fixed))
  )
  return(results)
}

scrambleFeatures <- function(iterations = 100, target, k = 5) {
  chunk_size <- max(ceiling(0.1 * iterations), min(ceiling(0.2 * iterations), iterations %/% 2))
  if (chunk_size %% 2 != 0) chunk_size <- chunk_size + 1
  chunked_iterations <- split(1:iterations, ceiling(seq_along(1:iterations) / chunk_size))
  p <- progressr::progressor(along = seq_along(chunked_iterations))
  
  results <- map_dfr(seq_along(chunked_iterations), function(chunk_idx) {
    chunk <- chunked_iterations[[chunk_idx]]
    p(sprintf("Processing chunk %d/%d", chunk_idx, length(chunked_iterations)))
    future_map_dfr(chunk, ~ {
      set.seed(.x)
      all_features <- colnames(filtered_data)[!colnames(filtered_data) %in% c(target, "SalePrice")]
      random_features <- sample(all_features, sample(10:50, 1))
      
      knn_nb_results <- evaluateModels(random_features, target, k)
      regression_results <- evaluate_regression(random_features, filtered_data)
      
      combined_results <- tibble(
        features = regression_results$features,
        nb_accuracy = knn_nb_results$nb_accuracy,
        nb_sensitivity = knn_nb_results$nb_sensitivity,
        nb_specificity = knn_nb_results$nb_specificity,
        knn_accuracy = knn_nb_results$knn_accuracy,
        knn_sensitivity = knn_nb_results$knn_sensitivity,
        knn_specificity = knn_nb_results$knn_specificity,
        adjusted_r2 = regression_results$adjusted_r2,
        AIC = regression_results$AIC,
        BIC = regression_results$BIC
      )
      return(combined_results)
    }, .options = furrr_options(seed = TRUE))
  })
  
  return(results)
}

runTests <- function(runs = 10, k = 30, max = 20, target) {
  progressr::with_progress({
    nb_knn_results <- scrambleFeatures(runs, target, k)
  })
  
  fixed_results <- evaluate_fixed_models(filtered_data)
  
  all_results <- bind_rows(nb_knn_results, fixed_results) |>
    mutate(across(where(is.numeric), ~ round(.x, 6))) |>
    rename(
      `Features` = features,
      `NB Accuracy` = nb_accuracy,
      `NB Sensitivity` = nb_sensitivity,
      `NB Specificity` = nb_specificity,
      `KNN Accuracy` = knn_accuracy,
      `KNN Sensitivity` = knn_sensitivity,
      `KNN Specificity` = knn_specificity,
      `Adj. R2` = adjusted_r2
    )
  
  datatable(
    all_results,
    caption = "Performance Comparison for NB, KNN, and Regression Models",
    options = list(pageLength = max, autoWidth = TRUE),
    rownames = FALSE
  )
}
write.csv(all_results, "r_feature_selection.csv")


runTests(5000, 30, 30, "SalePriceBin")
